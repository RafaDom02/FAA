{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "489ea95a",
   "metadata": {},
   "source": [
    "# Apartado I (Naive-Bayes propio heart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e4f144",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Datos import Datos\n",
    "import EstrategiaParticionado\n",
    "import Clasificador\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593d81c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_1=Datos('./datasets/heart.csv')\n",
    "print(\"NOMINAL ATRIBUTOS HEART\")\n",
    "print(dataset_1.nominalAtributos)\n",
    "print(\"DICCIONARIO HEART\")\n",
    "print (dataset_1.diccionarios)\n",
    "print(\"MATRIZ DE DATOS HEART\")\n",
    "print(dataset_1.datos.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd6376e",
   "metadata": {},
   "outputs": [],
   "source": [
    "estrategiaVC=EstrategiaParticionado.ValidacionCruzada(5)\n",
    "estrategiaVC.creaParticiones(dataset_1.datos)       # datos se refiere a la matriz numérica calculada en Datos\n",
    "print(\"VALIDACIÓN CRUZADA HEART\")\n",
    "print(\"Train-particion 0: \", estrategiaVC.particiones[0].indicesTrain)       # particiones contiene la lista de particiones\n",
    "print(\"Test-particion 0: \", estrategiaVC.particiones[0].indicesTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b83e30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "estrategiaVS=EstrategiaParticionado.ValidacionSimple(5, 0.3)\n",
    "estrategiaVS.creaParticiones(dataset_1.datos)       # datos se refiere a la matriz numérica calculada en Datos\n",
    "print(\"VALIDACIÓN SIMPLE HEART\")\n",
    "print(\"Train-particion 0: \", estrategiaVS.particiones[0].indicesTrain)       # particiones contiene la lista de particiones\n",
    "print(\"Test-particion 0: \", estrategiaVS.particiones[0].indicesTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd7f530",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Validación Simple de datos HEART\")\n",
    "estrategia = EstrategiaParticionado.ValidacionSimple(5, 0.2)\n",
    "clasificador = Clasificador.Clasificador()\n",
    "nb1 = Clasificador.ClasificadorNaiveBayes()\n",
    "errores1 = clasificador.validacion(estrategia, dataset_1, nb1, None)\n",
    "nb2 = Clasificador.ClasificadorNaiveBayes(True)\n",
    "errores2 = clasificador.validacion(estrategia, dataset_1, nb2, None)\n",
    "\n",
    "headers = [\"Nº particion\", \"Indice error\"]\n",
    "\n",
    "desviacion_estandar = np.std(errores1)\n",
    "media = sum(errores1) / len(errores1)\n",
    "print(\"Indice de errores por partición:\")\n",
    "table = tabulate(enumerate(errores1, 1), headers, tablefmt=\"grid\")\n",
    "print(table)\n",
    "print(\"Desviación estandar: \" + str(desviacion_estandar) + \", media: \" + str(media))\n",
    "print(50*\"#\")\n",
    "\n",
    "desviacion_estandar = np.std(errores2)\n",
    "media = sum(errores2) / len(errores2)\n",
    "print(\"Indice de errores por partición (con corrección La Place):\")\n",
    "table_VSHP = tabulate(enumerate(errores2, 1), headers, tablefmt=\"grid\")\n",
    "print(table_VSHP)\n",
    "print(\"Desviación estandar: \" + str(desviacion_estandar) + \", media: \" + str(media))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79296b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Validación cruzada de datos HEART\")\n",
    "estrategia = EstrategiaParticionado.ValidacionCruzada(5)\n",
    "clasificador = Clasificador.Clasificador()\n",
    "nb1 = Clasificador.ClasificadorNaiveBayes()\n",
    "errores1 = clasificador.validacion(estrategia, dataset_1, nb1, None)\n",
    "nb2 = Clasificador.ClasificadorNaiveBayes(True)\n",
    "errores2 = clasificador.validacion(estrategia, dataset_1, nb2, None)\n",
    "\n",
    "desviacion_estandar = np.std(errores1)\n",
    "media = sum(errores1) / len(errores1)\n",
    "print(\"Indice de errores por partición:\")\n",
    "table = tabulate(enumerate(errores1, 1), headers, tablefmt=\"grid\")\n",
    "print(table)\n",
    "print(\"Desviación estandar: \" + str(desviacion_estandar) + \", media: \" + str(media))\n",
    "print(50*\"#\")\n",
    "\n",
    "desviacion_estandar = np.std(errores2)\n",
    "media = sum(errores2) / len(errores2)\n",
    "print(\"Indice de errores por partición (con corrección La Place):\")\n",
    "table = tabulate(enumerate(errores2, 1), headers, tablefmt=\"grid\")\n",
    "data_VCHP = errores2\n",
    "print(table)\n",
    "print(\"Desviación estandar: \" + str(desviacion_estandar) + \", media: \" + str(media))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379c7f6c",
   "metadata": {},
   "source": [
    "# Apartado I (Naive-Bayes propio TIC-TAC-TOE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8341f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_2=Datos('./datasets/tic-tac-toe.csv')\n",
    "print(\"NOMINAL ATRIBUTOS TIC-TAC-TOE\")\n",
    "print(dataset_2.nominalAtributos)\n",
    "print(\"DICCIONARIO TIC-TAC-TOE\")\n",
    "print (dataset_2.diccionarios)\n",
    "print(\"MATRIZ DE DATOS TIC-TAC-TOE\")\n",
    "print(dataset_2.datos.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bb31cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "estrategiaVC=EstrategiaParticionado.ValidacionCruzada(5)\n",
    "estrategiaVC.creaParticiones(dataset_2.datos)       # datos se refiere a la matriz numérica calculada en Datos\n",
    "print(\"VALIDACIÓN CRUZADA TIC-TAC-TOE\")\n",
    "print(\"Train-particion 0: \", estrategiaVC.particiones[0].indicesTrain)       # particiones contiene la lista de particiones\n",
    "print(\"Test-particion 0: \", estrategiaVC.particiones[0].indicesTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ea8ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "estrategiaVS=EstrategiaParticionado.ValidacionSimple(5, 0.3)\n",
    "estrategiaVS.creaParticiones(dataset_1.datos)       # datos se refiere a la matriz numérica calculada en Datos\n",
    "print(\"VALIDACIÓN SIMPLE TIC-TAC-TOE\")\n",
    "print(\"Train-particion 0: \", estrategiaVS.particiones[0].indicesTrain)       # particiones contiene la lista de particiones\n",
    "print(\"Test-particion 0: \", estrategiaVS.particiones[0].indicesTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0760ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Validación simple de datos TIC-TAC-TOE\")\n",
    "\n",
    "estrategia = EstrategiaParticionado.ValidacionSimple(5, 0.2)\n",
    "clasificador = Clasificador.Clasificador()\n",
    "nb1 = Clasificador.ClasificadorNaiveBayes()\n",
    "errores1 = clasificador.validacion(estrategia, dataset_2, nb1, None)\n",
    "nb2 = Clasificador.ClasificadorNaiveBayes(True)\n",
    "errores2 = clasificador.validacion(estrategia, dataset_2, nb2, None)\n",
    "\n",
    "desviacion_estandar = np.std(errores1)\n",
    "media = sum(errores1) / len(errores1)\n",
    "print(\"Indice de errores por partición:\")\n",
    "table = tabulate(enumerate(errores1, 1), headers, tablefmt=\"grid\")\n",
    "print(table)\n",
    "print(\"Desviación estandar: \" + str(desviacion_estandar) + \", media: \" + str(media))\n",
    "print(50*\"#\")\n",
    "\n",
    "desviacion_estandar = np.std(errores2)\n",
    "media = sum(errores2) / len(errores2)\n",
    "print(\"Indice de errores por partición (con corrección La Place):\")\n",
    "table_VSTP = tabulate(enumerate(errores2, 1), headers, tablefmt=\"grid\")\n",
    "print(table_VSTP)\n",
    "print(\"Desviación estandar: \" + str(desviacion_estandar) + \", media: \" + str(media))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c639b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Validación cruzada de datos TIC-TAC-TOE\")\n",
    "\n",
    "estrategia = EstrategiaParticionado.ValidacionCruzada(5)\n",
    "clasificador = Clasificador.Clasificador()\n",
    "nb1 = Clasificador.ClasificadorNaiveBayes()\n",
    "errores1 = clasificador.validacion(estrategia, dataset_2, nb1, None)\n",
    "nb2 = Clasificador.ClasificadorNaiveBayes(True)\n",
    "errores2 = clasificador.validacion(estrategia, dataset_2, nb2, None)\n",
    "\n",
    "desviacion_estandar = np.std(errores1)\n",
    "media = sum(errores1) / len(errores1)\n",
    "print(\"Indice de errores por partición:\")\n",
    "table = tabulate(enumerate(errores1, 1), headers, tablefmt=\"grid\")\n",
    "print(table)\n",
    "print(\"Desviación estandar: \" + str(desviacion_estandar) + \", media: \" + str(media))\n",
    "print(50*\"#\")\n",
    "\n",
    "desviacion_estandar = np.std(errores2)\n",
    "media = sum(errores2) / len(errores2)\n",
    "print(\"Indice de errores por partición (con corrección La Place):\")\n",
    "table = tabulate(enumerate(errores2, 1), headers, tablefmt=\"grid\")\n",
    "data_VCTP = errores2\n",
    "print(table)\n",
    "print(\"Desviación estandar: \" + str(desviacion_estandar) + \", media: \" + str(media))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a36b354",
   "metadata": {},
   "source": [
    "### Breve análisis de los resultados anteriores. Discutir el efecto Laplace\n",
    "Como se puede observar en ambos datasets, la validación simple suele presentar indices de fallos significativamente menores que la validación cruzada en ambos sets de datos, y para el dataset tik-tak-toe, cuando se presenta un dato con valor 0, esta corrección ayuda a rebajar el indice de fallo sutilmente en la posterior validación."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bf1944",
   "metadata": {},
   "source": [
    "# Apartado II (Naive-Bayes Scikit-Learn con TIC-TAC-TOE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fdc900",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Intento de OneHotEncoder que no salio bien :(\n",
    "\"\"\"def preprocess(dataset: Datos):\n",
    "    data_wo_lastColumn = dataset.datos.iloc[:,:-1]\n",
    "    data_lastColumn = dataset.datos.iloc[:,-1]\n",
    "\n",
    "    print(data_lastColumn)\n",
    "\n",
    "    encoder = OneHotEncoder(sparse=False, categories='auto')\n",
    "    dwlc_encoded = np.array(encoder.fit(data_wo_lastColumn))\n",
    "    aux = np.hstack((dwlc_encoded, data_lastColumn[np.newaxis, :]))\n",
    "    dataset.datos = np.zeros(aux.shape)\n",
    "    dataset.datos = aux\n",
    "    return dataset\n",
    "\n",
    "# Nuevo dataset de tic-tac-toe\n",
    "new_dataset_2 = preprocess(dataset_2)\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb68ac3e",
   "metadata": {},
   "source": [
    "### Multinomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f1afbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "estrategia = EstrategiaParticionado.ValidacionCruzada(5)\n",
    "clasificador_nbsk_lp = Clasificador.ClasificadorNaiveBayesSKLearn(1)\n",
    "clasificador_nbsk_nlp = Clasificador.ClasificadorNaiveBayesSKLearn(1,LaPlace= False)\n",
    "\n",
    "errores1 = clasificador.validacion(estrategia, dataset_2, clasificador_nbsk_lp)\n",
    "errores2 = clasificador.validacion(estrategia, dataset_2, clasificador_nbsk_nlp)\n",
    "\n",
    "desviacion_estandar = np.std(errores1)\n",
    "media = sum(errores1) / len(errores1)\n",
    "print(\"Indice de errores por partición:\")\n",
    "table = tabulate(enumerate(errores1, 1), headers, tablefmt=\"grid\")\n",
    "print(table)\n",
    "print(\"Desviación estandar: \" + str(desviacion_estandar) + \", media: \" + str(media))\n",
    "print(50*\"#\")\n",
    "\n",
    "desviacion_estandar = np.std(errores2)\n",
    "media = sum(errores2) / len(errores2)\n",
    "print(\"Indice de errores por partición (con corrección La Place):\")\n",
    "table = tabulate(enumerate(errores2, 1), headers, tablefmt=\"grid\")\n",
    "data_VCTM = errores2\n",
    "print(table)\n",
    "print(\"Desviación estandar: \" + str(desviacion_estandar) + \", media: \" + str(media))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2597c3c4",
   "metadata": {},
   "source": [
    "### Gausiana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb91954",
   "metadata": {},
   "outputs": [],
   "source": [
    "clasificador_nbsk_lp = Clasificador.ClasificadorNaiveBayesSKLearn(2)\n",
    "clasificador_nbsk_nlp = Clasificador.ClasificadorNaiveBayesSKLearn(2,LaPlace= False)\n",
    "\n",
    "errores1 = clasificador.validacion(estrategia, dataset_2, clasificador_nbsk_lp)\n",
    "errores2 = clasificador.validacion(estrategia, dataset_2, clasificador_nbsk_nlp)\n",
    "\n",
    "desviacion_estandar = np.std(errores1)\n",
    "media = sum(errores1) / len(errores1)\n",
    "print(\"Indice de errores por partición:\")\n",
    "table = tabulate(enumerate(errores1, 1), headers, tablefmt=\"grid\")\n",
    "print(table)\n",
    "print(\"Desviación estandar: \" + str(desviacion_estandar) + \", media: \" + str(media))\n",
    "print(50*\"#\")\n",
    "\n",
    "desviacion_estandar = np.std(errores2)\n",
    "media = sum(errores2) / len(errores2)\n",
    "print(\"Indice de errores por partición (con corrección La Place):\")\n",
    "table = tabulate(enumerate(errores2, 1), headers, tablefmt=\"grid\")\n",
    "data_VCTG = errores2\n",
    "print(table)\n",
    "print(\"Desviación estandar: \" + str(desviacion_estandar) + \", media: \" + str(media))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2a1daa",
   "metadata": {},
   "source": [
    "### Categorica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed677d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "clasificador_nbsk_lp = Clasificador.ClasificadorNaiveBayesSKLearn(3)\n",
    "clasificador_nbsk_nlp = Clasificador.ClasificadorNaiveBayesSKLearn(3,LaPlace= False)\n",
    "\n",
    "errores1 = clasificador.validacion(estrategia, dataset_2, clasificador_nbsk_lp)\n",
    "errores2 = clasificador.validacion(estrategia, dataset_2, clasificador_nbsk_nlp)\n",
    "\n",
    "desviacion_estandar = np.std(errores1)\n",
    "media = sum(errores1) / len(errores1)\n",
    "print(\"Indice de errores por partición:\")\n",
    "table = tabulate(enumerate(errores1, 1), headers, tablefmt=\"grid\")\n",
    "print(table)\n",
    "print(\"Desviación estandar: \" + str(desviacion_estandar) + \", media: \" + str(media))\n",
    "print(50*\"#\")\n",
    "\n",
    "desviacion_estandar = np.std(errores2)\n",
    "media = sum(errores2) / len(errores2)\n",
    "print(\"Indice de errores por partición (con corrección La Place):\")\n",
    "table = tabulate(enumerate(errores2, 1), headers, tablefmt=\"grid\")\n",
    "data_VCTC = errores2\n",
    "print(table)\n",
    "print(\"Desviación estandar: \" + str(desviacion_estandar) + \", media: \" + str(media))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d79e71",
   "metadata": {},
   "source": [
    "## Analizar las diferencias de los resultados obtenidos en función del método empleado: MultinomialNB, GaussianNB y CategoricalNB. ¿Cuál ha funcionado mejor para tic-tac-toe? ¿A qué puede deberse?\n",
    "\n",
    "La mejor clasificacion es la de Multinomial, esto se debe a que debido a la naturaleza de los datos, es mejor elegir entre categorico o multinomial, pero debido a las combinaciones de x y o, es mejor la multinomial en este caso."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86c7922",
   "metadata": {},
   "source": [
    "# Apartado II (Naive-Bayes Scikit-Learn con HEART)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6d6da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "estrategia = EstrategiaParticionado.ValidacionCruzada(5)\n",
    "clasificador_nbsk_lp = Clasificador.ClasificadorNaiveBayesSKLearn(2)\n",
    "clasificador_nbsk_nlp = Clasificador.ClasificadorNaiveBayesSKLearn(2,LaPlace= False)\n",
    "\n",
    "errores1 = clasificador.validacion(estrategia, dataset_1, clasificador_nbsk_lp)\n",
    "errores2 = clasificador.validacion(estrategia, dataset_1, clasificador_nbsk_nlp)\n",
    "\n",
    "desviacion_estandar = np.std(errores1)\n",
    "media = sum(errores1) / len(errores1)\n",
    "print(\"Indice de errores por partición:\")\n",
    "table = tabulate(enumerate(errores1, 1), headers, tablefmt=\"grid\")\n",
    "print(table)\n",
    "print(\"Desviación estandar: \" + str(desviacion_estandar) + \", media: \" + str(media))\n",
    "print(50*\"#\")\n",
    "\n",
    "desviacion_estandar = np.std(errores2)\n",
    "media = sum(errores2) / len(errores2)\n",
    "print(\"Indice de errores por partición (con corrección La Place):\")\n",
    "table = tabulate(enumerate(errores2, 1), headers, tablefmt=\"grid\")\n",
    "data_VCH = errores2\n",
    "print(table)\n",
    "print(\"Desviación estandar: \" + str(desviacion_estandar) + \", media: \" + str(media))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13627467",
   "metadata": {},
   "source": [
    "### ¿Por qué crees que no utilizamos los otros dos algoritmos aquí? Si es por algún problema en los datos de entrada, ¿qué transformación/es podríamos hacer para resolver el problema?\n",
    "\n",
    "Este algoritmo es adecuado para variables numéricas. Si las variables numéricas como \"Age\", \"RestingBP\", \"Cholesterol\", \"MaxHR\" y \"Oldpeak\" son las que más influyen en la variable \"Class\". Mientras que Multinomial y Categorico con las varaibles \"Sex\", \"ChestPainType\", \"FastingBS\", \"RestingECG\", \"ExerciseAngina\" y \"ST_Slope\" no es tan efectivo para \"Class\".\n",
    "\n",
    "Sí, la transformación one-hot resolveria este problema para poder usar estos dos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7fe4c2f",
   "metadata": {},
   "source": [
    "# APARTADO III (CONCLUSIÓN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d440ee46",
   "metadata": {},
   "source": [
    "Nuestra implementación es mejor que la de sklearn, ya que tenemos en cuenta si una columna es gaussiana o multinomial, mientras que en sklearn aplicamos para todos los datos MultinomialNB o GaussianNB, lo cual nos cuasa una tasa de error mayor.\n",
    "- A continuación se muestra una tabla con los indices de fallo tanto de nuestra implementación como de las libreria sklearn por cada partición de test: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd94bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [data_VCTP, data_VCTM, data_VCTG, data_VCTC, data_VCHP, data_VCH]\n",
    "# Transpose the data to have one list per column\n",
    "data_transposed = list(map(list, zip(*data)))\n",
    "\n",
    "# Use tabulate to format the transposed data\n",
    "table = tabulate(data_transposed, headers=[\"Vc Tic-Tac-Toe (Propio)\", \"Vc Tic-Tac-Toe (Multinomial)\", \"Vc Tic-Tac-Toe (Gausiana)\", \"Vc Tic-Tac-Toe (Categorica)\" , \"Vc Heart (Propia)\", \"Vc Heart (scikit-learn)\"], tablefmt=\"grid\")\n",
    "\n",
    "print(table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
